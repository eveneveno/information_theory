\relax 
\providecommand\tcolorbox@label[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Information Measures}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Independence and Markov Chain}{1}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.1 (Independence)}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Conceptually, when $X \perp Z \mid Y$, $X, Y, Z$ are related as above.}}{1}}
\newlabel{fig:my_label}{{1}{1}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.2 (Mutual Independence)}{1}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.3 (Pairwise Independence)}{1}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.4 (Conditional Independence)}{1}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.5}{1}}
\@writefile{toc}{\contentsline {paragraph}{Proof A.}{1}}
\@writefile{toc}{\contentsline {paragraph}{Proof B.}{1}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.6 (Markov Chain)}{2}}
\@writefile{toc}{\contentsline {paragraph}{Remark.}{2}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.7}{2}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.8}{2}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.9}{2}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.10 (Markov subchains)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Markov subchains.}}{2}}
\newlabel{fig:my_label}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Shannon's Information Measures}{3}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.13 (Entropy.)}{3}}
\@writefile{toc}{\contentsline {paragraph}{Example (Binary Entropy Function).}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Binary Entropy Function.}}{3}}
\newlabel{fig:my_label}{{3}{3}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.14 (Joint Entropy)}{3}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.15 (Conditional Entropy)}{3}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.16}{3}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.17 (Mutual Information)}{4}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{4}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{4}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.18}{4}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.19}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Information Diagram.}}{4}}
\newlabel{fig:my_label}{{4}{4}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.20}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Continuity of Shannon's Information Measures for Fixed Finite Alphabets}{5}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.23 (Variational Distance)}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Chain Rules}{6}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.24(Chain Rule for Entropy)}{6}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.25 (Chain Rule for Conditional Entropy)}{6}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.26 (Chain Rule for Mutual Information)}{6}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.27 (Chain Rule for Conditional Mutual Information)}{6}}
\@writefile{toc}{\contentsline {paragraph}{Alternative Proof of Proposition 2.25}{6}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Information Divergence}{7}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.28 (Information Divergence)}{7}}
\@writefile{toc}{\contentsline {paragraph}{Convention:}{7}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 2.29 (Fundamental Inequality)}{7}}
\@writefile{toc}{\contentsline {paragraph}{Corollary 2.30}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Information Diagram.}}{7}}
\newlabel{fig:my_label}{{5}{7}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.31 (Divergence Inequality)}{8}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{8}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.32 (Log-Sum Inequality)}{8}}
\@writefile{toc}{\contentsline {paragraph}{Example:}{8}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{9}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{9}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.33 (Pinsker's Inequality)}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Basic Inequalities}{10}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.34}{10}}
\@writefile{toc}{\contentsline {paragraph}{Corollary}{10}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{10}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.35}{10}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.36}{10}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 2.37}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Some Useful Information Inequalities}{11}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.38 (Conditioning Does Not Increase Entropy)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{11}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.39 (Independence Bound for Entropy)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{11}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.40}{11}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{11}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 2.41}{12}}
\@writefile{toc}{\contentsline {paragraph}{Corollary}{12}}
\@writefile{toc}{\contentsline {paragraph}{Proof Corollary}{12}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{12}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{12}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.42 (\leavevmode {\color  {blue}Data Processing Theorem})}{12}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Fano's Inequality}{13}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.43}{13}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{13}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{13}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 2.47 (Fano's Inequality)}{13}}
\@writefile{toc}{\contentsline {paragraph}{Corollary 2.48}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Entropy Rate of a Stationary Source}{13}}
\@writefile{toc}{\contentsline {paragraph}{Definition 2.54}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2}The I-Measure}{14}}
\@writefile{toc}{\contentsline {paragraph}{1. Examples}{14}}
\@writefile{toc}{\contentsline {paragraph}{2. Inclusion-Exclusion formulation in set-theory}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Preliminaries}{14}}
\@writefile{toc}{\contentsline {paragraph}{Definition 3.1}{14}}
\@writefile{toc}{\contentsline {paragraph}{Definition 3.2}{14}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.3}{14}}
\@writefile{toc}{\contentsline {paragraph}{Definition 3.4}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Construction of the I-Measure $\mu ^{*}$}{15}}
\@writefile{toc}{\contentsline {paragraph}{Notations}{15}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 3.6}{15}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{15}}
\@writefile{toc}{\contentsline {paragraph}{The Inclusion-Exclusion Formula}{15}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 3.19 (Variation of the Inclusion-Exclusion Formula)}{15}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Lemma 3.7}{15}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Lemma 3.8}{15}}
\@writefile{toc}{\contentsline {paragraph}{Construction of the $I$ -Measure $\mu ^{*}$ on $\mathcal  {F}_{n}$}{15}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 3.9}{15}}
\@writefile{toc}{\contentsline {paragraph}{Implications}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}$\mu ^{*}$ can be Negative}{16}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.10}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $\mu ^{*}$ can be Negative.}}{16}}
\newlabel{fig:my_label}{{6}{16}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 3.11}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Information Diagrams for Markov Chains}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Suppressed information diagram.}}{17}}
\newlabel{fig:my_label}{{7}{17}}
\@writefile{toc}{\contentsline {paragraph}{Illustration:}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Suppressed information diagram.}}{17}}
\newlabel{fig:my_label}{{8}{17}}
\@writefile{toc}{\contentsline {paragraph}{Illustration:}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Examples of Applications}{19}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.12 (Concavity of Entropy)}{19}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{19}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{19}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.13/3.14 (Convexity/Concavity of Mutual Information)}{19}}
\@writefile{toc}{\contentsline {paragraph}{Proof 3.13}{19}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{19}}
\@writefile{toc}{\contentsline {paragraph}{Proof 3.14}{19}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Systems}}{19}}
\newlabel{fig:my_label}{{9}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Shannon's Perfect Secrecy Theorem}}{20}}
\newlabel{fig:my_label}{{10}{20}}
\@writefile{toc}{\contentsline {paragraph}{Shannon's Perfect Secrecy Theorem}{20}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.15 (Imperfect Secrecy Theorem)}{20}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{20}}
\@writefile{toc}{\contentsline {paragraph}{Example 3.17 (Data Processing Theorem)}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Shannon's Perfect Secrecy Theorem}}{20}}
\newlabel{fig:my_label}{{11}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Zero-Error Data Compression}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Entropy Bound}{21}}
\@writefile{toc}{\contentsline {paragraph}{Definition 4.1}{21}}
\@writefile{toc}{\contentsline {paragraph}{Definition 4.2}{21}}
\@writefile{toc}{\contentsline {paragraph}{Example 4.3}{21}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 4.4 (Kraft Inequality)}{21}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{21}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 4.6 (Entropy Bound)}{22}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{22}}
\@writefile{toc}{\contentsline {paragraph}{Corollary 4.7 (Theorem 2.43)}{22}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{23}}
\@writefile{toc}{\contentsline {paragraph}{Definition 4.8}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Prefix Codes}{24}}
\@writefile{toc}{\contentsline {paragraph}{Definition 4.9}{24}}
\@writefile{toc}{\contentsline {paragraph}{Code Tree for Prefix Code}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Instantaneous Decoding}}{24}}
\newlabel{fig:my_label}{{12}{24}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 4.11}{24}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{24}}
\@writefile{toc}{\contentsline {paragraph}{Definition ($D$-adic distribution)}{25}}
\@writefile{toc}{\contentsline {paragraph}{Corollary 4.12}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Huffman Codes}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Instantaneous Decoding}}{25}}
\newlabel{fig:my_label}{{13}{25}}
\@writefile{toc}{\contentsline {paragraph}{Optimality of Huffman Codes}{25}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 4.17}{26}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 4.15}{26}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 4.16}{26}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Strong Typicality}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Strong AEP}{27}}
\@writefile{toc}{\contentsline {paragraph}{Definition 6.1}{27}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.2 (Strong AEP)}{27}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{27}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework}}{29}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Proof.}}{29}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.3.}{29}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 6.4 (Chernoff Bound).}{30}}
\@writefile{toc}{\contentsline {paragraph}{Proof}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Strong Typicality Versus Weak Typicality}{31}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 6.5.}{31}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Joint Typicality}{31}}
\@writefile{toc}{\contentsline {paragraph}{Definition $6.6 .$}{31}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.7 (Consistency).}{31}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{32}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.8 (Preservation).}{32}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{32}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.9 (Strong JAEP).}{33}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 6.10 (Conditional Strong AEP).}{33}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{33}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 6.11.}{34}}
\@writefile{toc}{\contentsline {paragraph}{Corollary 6.12.}{34}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces In this array, the rows and the columns are the typical sequences $\mathbf  {x} \in S_{[X] \delta }^{n}$ and $\mathbf  {y} \in S_{[Y] \delta }^{n}$ respectively. The total number of rows and columns are approximately equal to $2^{n H(X)}$ and $2^{n H(Y)},$ respectively. An entry indexed by $(\mathbf  {x}, \mathbf  {y})$ receives a dot if $(\mathbf  {x}, \mathbf  {y})$ is strongly jointly typical. The total number of dots is approximately equal to $2^{n H(X, Y)}$. The number of dots in each row is approximately equal to $2^{n H(Y \mid X)},$ while the number of dots in each column is approximately equal to $2^{n H(X \mid Y)}$}}{35}}
\newlabel{fig:my_label}{{14}{35}}
\@writefile{toc}{\contentsline {paragraph}{Proposition 6.13.}{35}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework (Proof.)}}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}An Interpretation of the Basic Inequalities}{35}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework}}{37}}
\@writefile{toc}{\contentsline {paragraph}{Counter Example}{37}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}5. Alternative definition of weak typicality.}}{37}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework Alternative definition of strong typicality.}}{37}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{37}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework}}{37}}
\@writefile{toc}{\contentsline {paragraph}{Proof.}{37}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Homework}}{37}}
\@writefile{toc}{\contentsline {paragraph}{\leavevmode {\color  {red}Proof}}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discrete Memoryless Channels}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}7.1 Definition and Capacity}{39}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.1.}{39}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.2.}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Illustrations of (a) a discrete channel $p(y|x)$ and (b) a discrete channel $(\alpha , Z)$.}}{39}}
\newlabel{fig:my_label}{{15}{39}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.3.}{39}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.5.}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.6.}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}The Channel Coding Theorem}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.9.}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.10.}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.11.}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.12.}{40}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.13.}{41}}
\@writefile{toc}{\contentsline {paragraph}{Definition 7.14.}{41}}
\@writefile{toc}{\contentsline {paragraph}{Theorem 7.15 (Channel Coding Theorem).}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}The Converse}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The dependency graph for a channel code without feedback.}}{41}}
\newlabel{fig:my_label}{{16}{41}}
\@writefile{toc}{\contentsline {paragraph}{Lemma 7.16.}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Achievability}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}A Discussion}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Feedback Capacity}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Separation of Source and Channel Coding}{43}}
